# Week summary


## Week 2: Programming Exercise 1: Linear Regression

Linear regression on population data. You calculate a cost function and gradient descent function to calculate the coefficients. You see how the cost function varies with the fit coefficients. You end up making a plot of your prediction.

## Week 3: Programming Exercise 2: Logistic Regression

Binary logistic regression on exam data. Calculate a sigmoid function (used for cost), cost function, predict function, and regulatized cost function. fminunc is used to calculate fit variables $\theta$ (by minimizing the cost function). You draw a boundary line for the two predictors and make a prediction on whether or not people will get admitted or not. The probability of accceptance is the sigmoid function $h(z)$ evaluated at $ h( \theta * x )$. The probability is from 0 to 1. If it's higher than 0.5, you say they are admitted with a probability of x, where x is the predict value. This is how you test your prediction against actual data.

There was a bonus regularization exercise. They create a nonlinear (polynomial) model with some number of fit coefficients. You write the cost function and gradient discent for them. The $\lambda$ regulatization parameter is an input parameter. You can vary $\lambda$ to see overfitting (small $\lambda$) and underfitting (high $\lambda$ ).

## Week 4: Programming Exercise 3: Mult-class Classification and Neural Networks

## Week 5: Programming Exercise 4: Neural Networks Learning

## Week 6: Programming Exercise 5: Regularized Linear Regression and Bias vs. Variance

## Week 7: Programming Exercise 6: Support Vector Machines

## Week 8: Programming Exercise 7: K-means Clustering and Principal Component Analysis

## Week 9: Programming Exercise 8: Amonmaly Dection and Recommmender Systems

